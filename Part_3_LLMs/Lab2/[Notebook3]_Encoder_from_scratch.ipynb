{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny Transformer Classifier from Scratch\n",
        "The objective of this notebook is to train a LLM from scratch on a small dataset for text classification. The model will be trained directly on the *text classification* task. The main topics we will be working on:\n",
        "1. Data preparation\n",
        "2. Building the LLM architecture\n",
        "3. Training an LLM\n",
        "\n",
        "We start with the usual library imports."
      ],
      "metadata": {
        "id": "KC69SjC-ogv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "JZ4n6hxMHsnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-only architecture\n",
        "Since we are training a *transformer model* for the task of text classification, an *encoder-only* transformer will suffice. This will allow us to focus on the content seen in today's lecture without having to worry about the decoder part.\n",
        "\n",
        "Our encoder-only model will be constituted of different modules and sub-modules, namely:\n",
        "1. An *embedding module*, formed by:\n",
        "  - An *input embedding* sub-module.\n",
        "  - A *positional encoding* sub-module.\n",
        "2. A *transformer encoder block*, formed by:\n",
        "  - A *mulit-head (self) attention* sub-module.\n",
        "  - A couple of *Layer Norm* layers.\n",
        "  - A *feed forward* sub-module.\n",
        "  - *Residual connections*.\n",
        "3. A *classification head*, formed by a linear layer.\n",
        "\n",
        "Next we will implement the main building blocks of the *Transformer encoder block*."
      ],
      "metadata": {
        "id": "mOQhmXVxpH4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention\n",
        "The *multi-head* attention layer takes the input and processes it in chunks of equal length through eahc of its different heads, by using the attention mechanism, i.e. computing the queries, keys and values, and the attention scores and weights from them.\n",
        "\n",
        "**Exercise.** Implement the `MultiHeadAttention` by filling in the `TODO` flags below."
      ],
      "metadata": {
        "id": "0qjxg1HHQ0GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super().__init__()\n",
        "        # TODO: use the 'assert' statement to check that d_out is divisible by num_heads\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = ...  # TODO: Compute the per-head hidden dimension\n",
        "\n",
        "        self.W_query = ... # TODO: initilize the linear layer for the queries with the appropriate dimensions\n",
        "        self.W_key = ... # TODO: initilize the linear layer for the keys with the appropriate dimensions\n",
        "        self.W_value = ... # TODO: initilize the linear layer for the values with the appropriate dimensions\n",
        "\n",
        "        self.W_out = ... # TODO: initilize the linear layer for the output with the appropriate dimensions\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_tokens, hidden_dim = x.shape\n",
        "        assert hidden_dim == self.hidden_dim, f\"hidden_dim must be {self.hidden_dim}\"\n",
        "\n",
        "        keys = ... # TODO: apply the queries layer to the input # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        queries = ... # TODO: apply the keys layer to the input # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        values = ... # TODO: apply the values layer to the input # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (batch_size, num_tokens, d_out) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention)\n",
        "        attn_scores = torch.einsum(\"bijk, bikl -> bijl\", queries, keys.transpose(2, 3))\n",
        "        attn_weights = torch.softmax(attn_scores / (self.head_dim**0.5), dim=-1)\n",
        "\n",
        "        # Attention output\n",
        "        attn_output = torch.einsum(\"bijk, bikl -> bijl\", attn_weights, values)\n",
        "\n",
        "        # Transpose back: (batch_size, num_heads, num_tokens, head_dim) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Concatenate heads: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_tokens, hiddend_dim)\n",
        "        attn_output = attn_output.reshape(batch_size, num_tokens, self.hidden_dim)\n",
        "\n",
        "        # Compute output\n",
        "        output = self.W_out(attn_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "iI3BBVgeFJlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "fouT2vol6StY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads  # Compute the per-head hidden dimension\n",
        "\n",
        "        self.W_query = nn.Linear(hidden_dim, hidden_dim) # queries weight matrix\n",
        "        self.W_key = nn.Linear(hidden_dim, hidden_dim) # keys weight matrix\n",
        "        self.W_value = nn.Linear(hidden_dim, hidden_dim) # values weight matrix\n",
        "\n",
        "        self.W_out = nn.Linear(hidden_dim, hidden_dim)  # output weight matrix\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_tokens, hidden_dim = x.shape\n",
        "        assert hidden_dim == self.hidden_dim, f\"hidden_dim must be {self.hidden_dim}\"\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        queries = self.W_query(x) # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "        values = self.W_value(x) # Shape: (batch_size, num_tokens, hidden_dim)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (batch_size, num_tokens, d_out) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention)\n",
        "        attn_scores = torch.einsum(\"bijk, bikl -> bijl\", queries, keys.transpose(2, 3))\n",
        "        attn_weights = torch.softmax(attn_scores / (self.head_dim**0.5), dim=-1)\n",
        "\n",
        "        # Attention output\n",
        "        attn_output = torch.einsum(\"bijk, bikl -> bijl\", attn_weights, values)\n",
        "\n",
        "        # Transpose back: (batch_size, num_heads, num_tokens, head_dim) -> (batch_size, num_tokens, num_heads, head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Concatenate heads: (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_tokens, hiddend_dim)\n",
        "        attn_output = attn_output.reshape(batch_size, num_tokens, self.hidden_dim)\n",
        "\n",
        "        # Compute output\n",
        "        output = self.W_out(attn_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "7vjOACIv5Lrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Norm\n",
        "You might have heard of BatchNorm, where the inputs of a layer are normalized across the batch: the mean and standard deviation of the inputs are computed accross the batch dimension, then the inputs are normalized by substracting the mean and dividing by the standard deviation.\n",
        "\n",
        "The *LayerNorm* normalization technique is similar, only the normalization happens accross the feature dimension rather than the batch dimension.\n",
        "\n",
        "Once the feature mean $\\mu$ and standard deviation $\\sigma$ are computed, the inputs are normalized as follows:\n",
        "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}},$$\n",
        "where $\\epsilon$ is a small constant typically taken to be equal to $10^{-5}$ to avoid division by numbers close to zero.\n",
        "\n",
        "The output of the `LayerNorm` is not $\\hat{x}_i$ though, but\n",
        "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
        "where $\\gamma$ (scale parameter) and $\\beta$ (shift parameter) are learnable parameters.\n",
        "\n",
        "**Exercise.** Complete the `LayerNorm` class below by computing the feature mean and variance and performing the appropriate normalization."
      ],
      "metadata": {
        "id": "Tt5UkUGSQ281"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5 # small value to avoid division by zero\n",
        "        self.scale = nn.Parameter(torch.ones(hidden_dim)) # scale parameter (learnable)\n",
        "        self.shift = nn.Parameter(torch.zeros(hidden_dim)) # shift parameter (learnable)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = ... # TODO: compute the mean of the input tensor over the last dimension\n",
        "        var = ... # TODO: compute the variance of the input tensor over the last dimension\n",
        "        norm_x = ... # TODO: normalize the input tensor\n",
        "        y = ... # TODO: apply the learned scale and shift parameters\n",
        "        return y"
      ],
      "metadata": {
        "id": "pN4wBFhuN37C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "3L7751JB62-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5 # small value to avoid division by zero\n",
        "        self.scale = nn.Parameter(torch.ones(hidden_dim)) # scale parameter (learnable)\n",
        "        self.shift = nn.Parameter(torch.zeros(hidden_dim)) # shift parameter (learnable)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        y = self.scale * norm_x + self.shift\n",
        "        return y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ChlI3vxH6m9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed-Forward Network\n",
        "The *feed forward* sub-modules in our *transformer blocks* will consist of:\n",
        "1. A linear layer, the output dimension being 4 times `hidden_dim`.\n",
        "2. A ReLU activation\n",
        "3. A linear layer, the output dimension being `hidden_dim`.\n",
        "\n",
        "**Exercise.** Work out the input dimension of the two linear layers, and complete the `FeedForward` class by filling in the `TODO` flags."
      ],
      "metadata": {
        "id": "uz8bLo4OQ5RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # TODO: add the two linear layers with the intermediate ReLU activation function to the following sequential NN\n",
        "        self.layers = nn.Sequential(\n",
        "            ... # TODO: add the layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "8TApXigEPMRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "-lgvqZzn7-iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9uycJaSs72Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are done with the three main building blocks that form the *transformer encoder block*. The next steps are to implement the *transformer encoder block* itself as well as the *embedding sub-module*. Once those two sub-modules will be implemented, we will be ready to implement the final *end-to-end* transformer classifier model.\n"
      ],
      "metadata": {
        "id": "Pp1YZSsk8INF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Encoder Block\n",
        "As we have mentioned earlier, the *Transformer encoder block* is composed of:\n",
        "1. A *Multi-head attention* sub-module\n",
        "2. A *LayerNorm* normalization\n",
        "3. A *Feed Forward* sub-module\n",
        "4. A *LayerNorm* normalization\n",
        "\n",
        "Moreover, two skip-connections are present in the *Transformer Block*:\n",
        "- *(A)* A skip connection that adds the original input to the output of the operation *1* above.\n",
        "- *(B)* A skip connection that adds the output of the operation *2* to the output of the operation *3* above.\n",
        "\n",
        "**Exercise.** Implement the `TransformerEncoderBlock` module below by completing the `TODO` tags."
      ],
      "metadata": {
        "id": "Bnp_1xu3Q8Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_heads):\n",
        "    super(TransformerEncoderBlock, self).__init__()\n",
        "\n",
        "    self.attention = ... # TODO: initialize the multi-head attention module with the appropriate dimensions\n",
        "    self.norm1 = ... # TODO: initialize the first Layer Norm with the appropriate dimensions\n",
        "    self.norm2 = ... # TODO: initialize the second Layer Norm with the appropriate dimensions\n",
        "    self.feed_forward = ... # TODO: initialize the Feed Forward module with the appropriate dimensions\n",
        "\n",
        "  def forward(self, x):\n",
        "      # TODO: implement the forward pass of the TransformerEncoderBLock\n",
        "      return x"
      ],
      "metadata": {
        "id": "YwxZlVYCNYhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "L80ih1Dv922b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_heads):\n",
        "    super(TransformerEncoderBlock, self).__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(hidden_dim, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "    self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "    self.feed_forward = FeedForward(hidden_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "      attn_output = self.attention(x)\n",
        "      x = x + attn_output\n",
        "      x = self.norm1(x)\n",
        "      ff_output = self.feed_forward(x)\n",
        "      x = x + ff_output\n",
        "      x = self.norm2(x)\n",
        "      return x"
      ],
      "metadata": {
        "cellView": "form",
        "id": "izQHSy-o9y2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding\n",
        "As we have mentioned above, the *embedding sub-module* consists of both:\n",
        "- An *input embedding*: taking as input a token ID given by the tokenizer\n",
        "- A *positional encoding*: taking as input the position of a token in the sentence.\n",
        "\n",
        "**Question.** What is the input dimension of the *input embedding*? What about the *positional encoding*?\n",
        "\n",
        "**Exercise.** Implement the `Embedding` sub-module by completing the `TODO` tags below."
      ],
      "metadata": {
        "id": "ci9cZ_FzREGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, hidden_dim):\n",
        "    super().__init__()\n",
        "    # TODO: use the nn.Embedding layer to initialize the embedding and positional encoding below:\n",
        "    self.embedding = ... # TODO: initialize the input embedding with the appropriate dimensions\n",
        "    self.position_encoding = ... # TODO: initialize the positional encoding with the appropriate dimensions\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO: implement the forward pass of the Embedding"
      ],
      "metadata": {
        "id": "U2jq75MFTNrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "IMBwXCR_GNfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, hidden_dim):\n",
        "    super().__init__()\n",
        "    # TODO: use the nn.Embedding layer to initialize the embedding and positional encoding below:\n",
        "    self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "    self.position_encoding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    _, seq_length = x.shape\n",
        "    token_embeddings = self.embedding(x)\n",
        "    pos_encodings = self.position_encoding(torch.arange(seq_length, device=x.device))\n",
        "    return token_embeddings + pos_encodings"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tClizNveGIqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Encoder-Only Model for Classification\n",
        "We have implemented all necessary sub-modules ane we are now ready to implement the *end-to-end* architecture of the *transformer encoder-only* model. To that end, we will make use of:\n",
        "- The `Embedding` sub-module\n",
        "- The `TransformerEncoderBlock` sub-module\n",
        "- A `nn.Linear` layer to act as a classification head.\n",
        "\n",
        "**Note.** The output of the `TransformerEncoderBlock` sub-module is a tensor of shape $(b, s, d_h)$ where $b$ represents the batch size, $s$ the sequence length, and $d_h$ the hidden dimension of the model. In order to perform the classification task, we need to compute one logit per sequence in the batch. In order to do so, we use a strategy common to the *BERT family* of models: we only use the representation of the special token `[CLS]` as input to the classification head. Note that in order for this strategy to work, we will use a tokenizer that adds the token `[CLS]` at the beginning of each input token sequence.\n",
        "\n",
        "**Exercise.** Implement the `TransformerClassifier` model by completing the `TODO` tags below."
      ],
      "metadata": {
        "id": "M9X5IMMJQ_Q6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbdmGi3UEr-J"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               max_length,\n",
        "               hidden_dim,\n",
        "               num_heads,\n",
        "               num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = ... # TODO: initialize the embedding with the appropriate parameters\n",
        "    self.encoder = # TODO: initialize the encoder with the appropriate parameters\n",
        "    self.classifier_head = # TODO: initialize the classifier head with the appropriate parameters\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = ... # TODO: compute the embedding of x\n",
        "    x = ... # TODO: compute the encoding of x\n",
        "    x = x[:, 0, :] # We only use the encoding of the token [CLS] as input of the classification head.\n",
        "    x = ... # TODO: compute the logits through the classification head\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "1FI9jQ8dHLOe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5fkRH3b-HJhd"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "class TransformerClassifier(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               max_length,\n",
        "               hidden_dim,\n",
        "               num_heads,\n",
        "               num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = Embedding(vocab_size, max_length, hidden_dim)\n",
        "    self.encoder = TransformerEncoderBlock(hidden_dim, num_heads)\n",
        "    self.classifier_head = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.encoder(x)\n",
        "    x = x[:, 0, :]\n",
        "    x = self.classifier_head(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "We will train the `TransformerClassifier` model on a text classification task. The data set we will be using is the following [dataset](https://huggingface.co/datasets/dair-ai/emotion/viewer). It consists of sentences grouped into 6 different classes according to the main emotion they convey:\n",
        "\n",
        "| Class ID | Emotion |\n",
        "| -------- | ------- |\n",
        "| 0 | sadness |\n",
        "| 1 | joy |\n",
        "| 2 | love |\n",
        "| 3 | anger |\n",
        "| 4 | fear |\n",
        "| 5 | surprise |\n",
        "\n"
      ],
      "metadata": {
        "id": "xuKT_oHNUy9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WTJhm3L_UwMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEBDlG_dGPw2"
      },
      "source": [
        "**Questions.** Answer the following questions:\n",
        "1. What type of object is the ``raw_dataset`` object ?\n",
        "2. How many elements are there in the ``raw_dataset`` object ?\n",
        "3. What type of object is the ``raw_dataset[\"train\"]`` object ?\n",
        "4. Describe the ``raw_dataset[\"train\"]`` object.\n",
        "5. Are the train, validation and test datasets balanced ?\n",
        "\n",
        "**Exercise.** Print one of the elements of ``raw_dataset[\"train\"]``."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"dair-ai/emotion\")\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "HTwqz1OlWIZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: print one of the elements of raw_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "jjgYHlNTWQKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-trained Tokenizer\n",
        "In order to convert the textual data in the above datasets into a format that can be used as input for our models, we first *tokenize* the text using a pre-trained tokenizer."
      ],
      "metadata": {
        "id": "XdBHC_KyXlPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)"
      ],
      "metadata": {
        "id": "P_HxyJv8XoI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Let's find out more about the tokenizer we will be using. Write code in order to answer the following questions:\n",
        "1. What is the name of the tokenizer being used?\n",
        "2. What is the size of the vocabulary?\n",
        "3. What is the maximum model input length?\n",
        "4. What special tokens does the tokenizer use? What are their IDs?\n",
        "\n",
        "**Remark.** Check that the special token `[CLS]` is indeed one of the special tokens in the pre-trained tokenizer. What is its token ID?"
      ],
      "metadata": {
        "id": "wcgDdKIZLkM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: print the necessary information about the automatically load tokenizer"
      ],
      "metadata": {
        "id": "iMeEIYU5L5EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the raw input\n",
        "We next use the pre-trained tokenizer to tokenize the whole raw dataset:"
      ],
      "metadata": {
        "id": "zD9iQiwTYtuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "2tr4bIWgYw1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loaders\n",
        "Finally, we create three separate pytorch data loaders in order to easily iterate through them during the training and evaluation phases."
      ],
      "metadata": {
        "id": "98g_OGBmYlT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    tokenized_dataset[\"validation\"],\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "Fe3flrRGbvYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation\n",
        "Now that we have both our model architecture defined and our data prepared, we can proceed to the last phase of the lab project: training and evaluating the model."
      ],
      "metadata": {
        "id": "3aRl_d2Ib_c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize model\n",
        "We start by setting all the necessary arguments in order to instantiate the concrete model that we will be using for the classification task.\n",
        "\n",
        "**Exercise.** Set all the necessary arguments:\n",
        "- `VOCAB_SIZE`: the size of the vocabulary of the pre-trained tokenizer.\n",
        "- `MAX_LENGTH`: the maximum length of the token sequence of the pre-trained tokenizer.\n",
        "- `HIDDEN_DIM`: 256\n",
        "- `NUM_HEADS`: 8\n",
        "- `NUM_CLASSES`: The number of classes of the multi-class classification task.\n",
        "\n",
        "**Question.** Make sure that the hidden dimension is divisible by the number of heads."
      ],
      "metadata": {
        "id": "LeK4kl5DPgI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = ... # TODO: extract the vocabulary size from the tokenizer\n",
        "MAX_LENGTH = ... # TODO: extract the maximum sequence length from the tokenizer\n",
        "HIDDEN_DIM = ... # TODO: set the hidden dimension\n",
        "NUM_HEADS = ... # TODO: set the number of heads\n",
        "NUM_CLASSES = ... # TODO: extract the number of classes from the dataset\n",
        "\n",
        "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
        "print(f\"MAX_LENGTH: {MAX_LENGTH}\")\n",
        "print(f\"HIDDEN_DIM: {HIDDEN_DIM}\")\n",
        "print(f\"NUM_HEADS: {NUM_HEADS}\")\n",
        "print(f\"NUM_CLASSES: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "jcEj0ZXqcFQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "7n1W6NF4SFSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "MAX_LENGTH = tokenizer.model_max_length\n",
        "HIDDEN_DIM = 256\n",
        "NUM_HEADS = 8\n",
        "NUM_CLASSES = tokenized_dataset[\"train\"].features[\"label\"].num_classes\n",
        "\n",
        "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
        "print(f\"MAX_LENGTH: {MAX_LENGTH}\")\n",
        "print(f\"HIDDEN_DIM: {HIDDEN_DIM}\")\n",
        "print(f\"NUM_HEADS: {NUM_HEADS}\")\n",
        "print(f\"NUM_CLASSES: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t4VNFvFPR_GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Instantiate the `classifier` model using the above arguments."
      ],
      "metadata": {
        "id": "M9gTF43_Sxcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = ... # TODO: instantiate the model\n",
        "print(classifier)"
      ],
      "metadata": {
        "id": "_3jjvGVzclru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "pkw4fWokSGCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "classifier = TransformerClassifier(VOCAB_SIZE, MAX_LENGTH, HIDDEN_DIM, NUM_HEADS, NUM_CLASSES)\n",
        "print(classifier)"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "xHtaIeXHS5rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "We can now proceed to the model training phase. In order to do so, we will define two functions:\n",
        "- A `train_epoch` function that will train the model by iterating through the given data loader once.\n",
        "- An `evaluate` function that will evaluate the model on the given data loader."
      ],
      "metadata": {
        "id": "07SxarFxdYC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Iz6hM8BAlysm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ],
      "metadata": {
        "id": "9XTZavxbf1Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
        "\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ],
      "metadata": {
        "id": "TJ8Ou_dSf6zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us proceed to the actual training of the model.\n",
        "\n",
        "**Questions.** What loss function should we use to train the model?\n",
        "\n",
        "**Exercise.** Train the model by:\n",
        "- Setting the number of epochs to 2 and the learning rate to 0.001.\n",
        "- Completing the `TODO` tags below."
      ],
      "metadata": {
        "id": "DdTfTufeTgy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = ... # TODO: set the number of epochs\n",
        "LEARNING_RATE = ... # TODO: set the learning rate\n",
        "\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)\n",
        "criterion = ... # TODO: set the loss function\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = ... # TODO: train the model for one epoch\n",
        "    valid_loss, valid_acc = ... # TODO: evaluate the model on the validation set\n",
        "\n",
        "    epoch_time = time.time()\n",
        "\n",
        "    print(\"\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_time}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "1fXAk9Euf4E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution.** Click below to check the solution."
      ],
      "metadata": {
        "id": "9rjaKyKNSISe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train_epoch(classifier, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(classifier, val_loader, criterion)\n",
        "\n",
        "    epoch_time = time.time()\n",
        "\n",
        "    print(\"\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_time}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6eUJEH9zUSuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question.** Is the model overfitting? Should we keep training during more epochs?"
      ],
      "metadata": {
        "id": "ERqdjhblUm5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "Finally, we evaluate the model on the test set."
      ],
      "metadata": {
        "id": "bd9x3tEErDmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(classifier, test_loader, criterion)\n",
        "print(\"\")\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "bzucyYNNiAgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Try out the model in a bunch of sentences of your own."
      ],
      "metadata": {
        "id": "4ZJU6QNzUv21"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wtr7w5GiVrBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}