{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Training a GPT-2 model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to train a LLM from scratch on a short text. The model will be trained on the *next token generation* or *causal generation* task. The main topics we will be working on:\n",
    "1. Data preparation\n",
    "2. Building the LLM architecture\n",
    "3. Training an LLM\n",
    "\n",
    "This lab projects is heavily inspired by [S. Raschka's youtube lecture](https://www.youtube.com/watch?v=quh7z1q7-uc&t=16s) on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM input data\n",
    "The obective of this section is to prepare the data in order to train a LLM. We will be using a short poem book called *Lou Catounet Gascoun*, written in the gascon dialact of the occitan language, by [Guilhèm Adèr](https://en.wikipedia.org/wiki/Guilh%C3%A8m_Ad%C3%A8r). Preparing the data for using with the LLM will be done in two steps:\n",
    "1. Tokenizing the data.\n",
    "2. Preparing the input-output data batches for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Using a pre-trained tokenizer\n",
    "A tokenizer has two main roles:\n",
    "- Breaking text into smaller chunks of characters called tokens.\n",
    "- Mapping the text expressed in these chunks into a sequence of integers.\n",
    "Each token is assigned a unique integer, so that the token-integer mapping is bijective, and one can both encode a text into a sequence of integers and decode a sequence of integers into a text.\n",
    "\n",
    "The more common option is to use a pre-trained tokenizer. In this notebook we will use a tokenizer based on the Byte Pair Encoding (BPE) tokenization method. The BPE tokenization algorithm builds a set of tokens iteratively based on a corpus of texts:\n",
    "- It starts with an initial vocabulary of all characters present in the corpus.\n",
    "- It creates new tokens by merging existing ones: at each step, the most frequent pair of existing tokens is merged.\n",
    "- The algorithm stops when the desired vocabulary size is reached.\n",
    "\n",
    "Below we use a tokenizer that has been pre-trained for the GPT-2 model on a large corpus of texts. We use the `tiktoken` library for its particularly efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Write a few sentences in the language of your choice and use the tokenizer to encode and decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write text in the strings below and encode it using the tokenizer\n",
    "text = (\n",
    "    \"...\"\n",
    "    \"...\"\n",
    ")\n",
    "\n",
    "integers = ...  # TODO: encode the text using the tokenizer\n",
    "# TODO: print the sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/encode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: decode the sequence of integers using the tokenizer and print the resulting string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/decode.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Loading the data\n",
    "The objective of this section is to load the data from the text we will be using into batches appropriate for training. Since we are training our LLM for the *next token prediction* task, we will train our LLM on input-output pairs where the output contains the input shifted to the left by one token (like we deed for RNNs).\n",
    "\n",
    "In order to do so, we will split the text into chunks of length `max_length`. These chanks can potentially overlap, the starting point of each chunk is obtained by taking the starting point of the previous chunk and adding the `stride` parameter.\n",
    "\n",
    "**Exercise.** Complete the function `GPTDataset` below by filling in the `TODO` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # TODO: Tokenize the entire text\n",
    "        token_ids = ...\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length by moving the window by stride\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = ...  # TODO: choose the input tokens\n",
    "            target_chunk = ... # TODO: choose the target tokens\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function creates a dataset from the given text `txt` by splitting it into chunks and creating input-output pairs. Next, we create a second helper function `create_dataloader` that uses the function `GPTDataset` to create the dataset and arrange it into batches for training.\n",
    "\n",
    "**Exercise.** Complete the function `create_dataloader` below by filling in the `TODO` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = ... # TODO: initialize the GPT2 tokenizer\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = ... # TODO: initialize the GPTDataset\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"catounet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Total number of characters in the text:\\n {len(text)}\\n\")\n",
    "\n",
    "print(f\"An excerpt from the book:\\n {text[1_000:1_500]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Load the `text` using the `create_dataloader` function and the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating batched input-output pairs\n",
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the LLM architecture\n",
    "The LLM uses the transformer architecture, the main layer types that are present in such an architecture are:\n",
    "- Multi-head attention\n",
    "- Layer Norm\n",
    "- GeLU activation\n",
    "- Feed-forward\n",
    "These different layer types will be conbined in a *transformer block*, and several transformer blocks will be stacked on top of each other to compose the whole GPT-2 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Multi-head Attention\n",
    "The *multi-head* attention layer takes the input and processes it in chunks of equal length through eahc of its different heads, by using the attention mechanism, i.e. computing the queries, keys and values, and the attention scores and weights from them.\n",
    "\n",
    "**Exercise.** Implement the `MultiHeadAttention` by filling in the `TODO` flags below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise.** Complete the `TODO` flags below in order to define these four kind of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # TODO: use the 'assert' statement to check that d_out is divisible by num_heads\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = ...  # TODO: compute the per-head dimension in terms of d_out and num_heads\n",
    "\n",
    "        self.W_query = # TODO: initilize the linear layer for the query with the appropriate dimensions and the optional bias term\n",
    "        self.W_key = # TODO: initilize the linear layer for the key with the appropriate dimensions and the optional bias term\n",
    "        self.W_value = # TODO: initilize the linear layer for the value with the appropriate dimensions and the optional bias term\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = # TODO: apply the key matrix to the input  # Shape: (b, num_tokens, d_out)\n",
    "        queries = # TODO: apply the query matrix to the input\n",
    "        values = # TODO: apply the value matrix to the input\n",
    "\n",
    "        # We implicitly split the matrices by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/multihead.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Layer Norm\n",
    "\n",
    "You might have heard of BatchNorm, where the inputs of a layer are normalized across the batch: the mean and standard deviation of the inputs are computed accross the batch dimension, then the inputs are normalized by substracting the mean and dividing by the standard deviation. \n",
    "\n",
    "The *LayerNorm* normalization technique is similar, only the normalization happens accross the feature dimension rather than the batch dimension.\n",
    "\n",
    "Once the feature mean $\\mu$ and standard deviation $\\sigma$ are computed, the inputs are normalized as follows:\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}},$$\n",
    "where $\\epsilon$ is a small constant typically taken to be equal to $10^{-5}$ to avoid division by numbers close to zero.\n",
    "\n",
    "The output of the `LayerNorm` is not $\\hat{x}_i$ though, but\n",
    "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "where $\\gamma$ (scale parameter) and $\\beta$ (shift parameter) are learnable parameters.\n",
    "\n",
    "**Exercise.** Complete the `LayerNorm` class below by computing the feature mean and variance and performing the appropriate normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 # small value to avoid division by zero(\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # scale parameter (learnable)\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # shift parameter (learnable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = ... # TODO: compute the mean of the input tensor over the last dimension\n",
    "        var = ... # TODO: compute the variance of the input tensor over the last dimension\n",
    "        norm_x = ... # TODO: normalize the input tensor\n",
    "        y = ... # TODO: apply the learned scale and shift parameters\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/layernorm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. GELU\n",
    "The *GELU* or Gaussian Error Linear Unit is an activation function defined as\n",
    "$$\\text{GELU}(x) = x\\Phi(x)$$\n",
    "where $\\Phi(x)$ is the CDF of the standard normal distribution.\n",
    "In order to avoid computationally expensive calculations, the GELU is often approximated using\n",
    "$$\\text{GELU}(x)\\simeq 0.5 x \\Bigg(\n",
    "    1 + \\text{tanh}\\bigg(\n",
    "        \\sqrt{\\frac{2}{\\pi}}\\big(x + 0.044715 x^3\\big)\n",
    "    \\bigg)\n",
    "    \\Bigg).$$\n",
    "We will use this approximation in our definition.\n",
    "\n",
    "**Exercise.** Code the GELU activation function using the above approximation. Use the `torch` implementation of *tanh*, square root, $\\pi$ and the *power* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gelu = ... # TODO: implement the GELU activation function\n",
    "        return gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/gelu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. The Feed Forward blocks\n",
    "The *feed forward* layers in our *transformer blocks* will consist of:\n",
    "1. A linear layer\n",
    "2. A GELU activation\n",
    "3. A linear layer\n",
    "\n",
    "The feed forward layer has the same input and output dimensions, and the hidden dimension is equal to 4 times the input dimension.\n",
    "\n",
    "**Exercise.** Complete the `FeedForward` class by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # TODO: add the two linear layers with the intermediate GELU activation function to the following sequential model\n",
    "        self.layers = nn.Sequential(\n",
    "            ... # TODO: add the layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/feedforward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. The Transformer block\n",
    "The next step is to define our *Transformer Block*, which is composed of:\n",
    "1. A *LayerNorm* normalization\n",
    "2. A *Multi-head attention* layer\n",
    "3. A *Dropout* layer\n",
    "4. A *LayerNorm* normalization\n",
    "5. A *Feed Forward* layer\n",
    "6. A *Dropout* layer\n",
    "\n",
    "Moreover, two skip-connections are present in the *Transformer Block*:\n",
    "- *(A)* A skip connection that adds the original input to the output of the operation *3* above.\n",
    "- *(B)* A skip connection that adds the output of *(A)* to the output of the operation *6* above.\n",
    "\n",
    "In order to initialize all the necessary layers, a configuration dictionary `cfg` will be passed to the `TransformerBlock` at initialization. This configuration dictionary will contain the information about the different parameters necessary to define the transformer architecture, under the following keywords:\n",
    "- `vocab_size`: number of tokens in the vocabulary\n",
    "- `emb_dim`: embedding dimension\n",
    "- `context_length`: context length\n",
    "- `n_heads`: number of heads in Multy head attention\n",
    "- `drop_rate`: dropout rate\n",
    "- `qkv_bias`: boolean determining weather to add a bias to the key, query and value matrices\n",
    "- `n_layers`: number of transformer blocks to stack\n",
    "\n",
    "**Exercise.** Implement the `forward` method of the `TransformerBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg[\"emb_dim\"])\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implement the forward pass of the TransformerBlock by performing the necessary operations on the input x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/transformerblock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. The GPT-2 architecture\n",
    "We next implement the GPT architecture by specifying the necessary model parameters in the `GPT_CONFIG_124M` dictionary, and by stacking transformer blocks along with other necessary layers in the `GPTModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Number of tokens in the GPT-2 vocabulary\n",
    "    \"context_length\": 128,   # Number of tokens in the context window\n",
    "    \"emb_dim\": 768,   # Dimension of token embeddings\n",
    "    \"n_heads\": 12,           # Number of attention heads in the multi-head attention layers\n",
    "    \"n_layers\": 12,         # Number of transformer layers\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Whether to include bias in the Q, K, V linear layers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Checking the GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []\n",
    "\n",
    "txt1 = \"Le chat noir est sur la\"\n",
    "txt2 = \"Le soleil brille dans\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(44)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Generating text with the GPT-2 model\n",
    "In this section we will write a `generate_text` function that uses a GPT-2 model to generate text given an input sequence of tokens.\n",
    "\n",
    "**Exercise.** Complete the `generate_text` function below by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # TODO: Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = ... # TODO: crop the current context\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = # TODO: get the logits from the model\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # TODO: Apply softmax to get probabilities\n",
    "        probas = ...  # (batch, vocab_size)\n",
    "\n",
    "        # TODO: Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = ...  # (batch, 1)\n",
    "\n",
    "        # TODO: Append sampled index to the running sequence\n",
    "        idx = ... # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Bonjour, je suis\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_text(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the LLM\n",
    "The objective of this section is to train the LLM on the text `catounet.txt`. In order to check that everything is working properly, we will reload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to tokenize and detokenize text\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.1. Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the book data\n",
    "with open(\"catounet.txt\", \"r\") as f:\n",
    "    book = f.read()\n",
    "\n",
    "print(f\"An excerpt from the book:\\n {book[1_000:1_500]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Print the total number of characters and the total number of tokens in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = # TODO: compute the total number of characters in the book\n",
    "total_tokens = # TODO: compute the total number of tokens in the book\n",
    "\n",
    "print(f\"Total number of characters in the book: {total_characters}\")\n",
    "print(f\"Total number of tokens in the book: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/nb_tokens.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train and validation loaders\n",
    "Next we will split the text into a training and a validation set, by taking the first 90% of the characters for training and the last 10% for validation. We will also load these train/validation sets into their respective data loaders.\n",
    "\n",
    "**Exercise.** Create the training and validation data loaders by completing the `TODO` flags below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the book into training and validation data\n",
    "train_ratio = # TODO: define the proper ratio\n",
    "split_idx = # TODO: find the index of the character where the split should occur\n",
    "train_data = # TODO: choose the training characters\n",
    "val_data = # TODO: choose the validation characters\n",
    "print(len(train_data), len(val_data))\n",
    "\n",
    "# We set a seed for reproducibility\n",
    "torch.manual_seed(44)\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/train_val.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An optional check that the data loaders are working as expected\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Computing the loss\n",
    "Since we are training the GPT-2 model on the *next token prediction* task, the output of the model consists of the logits for the different tokens in the whole token vocabulary, i.e. at each step, we are solving a *classification* problem. Therefore we can train our model with the usual *cross-entropy loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next compute the initial training and validation losses for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Training and monitoring functions\n",
    "Before we start training the model, we define three helper functions that will help us train the model and monitor the evolution of the model during training.\n",
    "\n",
    "**Exercice.** Complete the `evaluate_model` function below by filling in the `TODO` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = # TODO: calculate the training loss\n",
    "        val_loss = # TODO: calculate the validation loss\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_and_print_sample` function will allow us to generate and print sample text during training to monitor the evolution of the model capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print a sample text\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Complete the `train_model` function by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # TODO: Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = ... # TODO: initialize to three empty lists\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # TODO: Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # TODO:  Reset loss gradients from previous batch iteration\n",
    "            loss = ...  # TODO: calculate the loss for the current batch\n",
    "            # TODO: Calculate loss gradients using backpropagation\n",
    "            # TODO: Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = ... # TODO: evaluate the model\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # TODO: Print a sample text after each epoch\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/training/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(44)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=val_data[:20], tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Plotting the loss histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Questions\n",
    "Think about the following questions.\n",
    "1. Did the model learn?\n",
    "2. What does the loss plot tell you? What is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
